{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6316e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[[0.4711, 0.2645, 0.2645],\n",
      "         [0.2645, 0.4711, 0.2645],\n",
      "         [0.2645, 0.2645, 0.4711]]])\n",
      "Output:\n",
      " tensor([[[-0.3374, -0.5534,  0.8631, -0.5376],\n",
      "         [-0.6592, -0.2661,  0.9557, -0.8329],\n",
      "         [-0.1743, -0.7239,  1.2220, -0.6736]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import TinyGPT\n",
    "from data import CharDataset, CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3618f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 53\n"
     ]
    }
   ],
   "source": [
    "# Example toy dataset\n",
    "text = \"abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "tok = CharTokenizer(text)\n",
    "print(\"Vocab size:\", tok.vocab_size)\n",
    "\n",
    "# Encode the full text into tokens\n",
    "data = torch.tensor(tok.encode(text), dtype=torch.long)\n",
    "\n",
    "# 90% train, 10% val split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "dataset = {\n",
    "    \"train\": train_data,\n",
    "    \"val\": val_data\n",
    "}\n",
    "\n",
    "block_size = 8\n",
    "vocab_size = tok.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439a9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, data, batch_size=4, block_size=8, device=\"cuda\"):\n",
    "    # split: \"train\" or \"val\"\n",
    "    # data: dict with {\"train\": tensor, \"val\": tensor}\n",
    "    \n",
    "    # pick the right dataset\n",
    "    data_split = data[split]\n",
    "\n",
    "    # pick random starting indices\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    \n",
    "    # slice out input (x) and target (y)\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c1d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Step 0, loss = 82.2274\n",
      "Step 100, loss = 6.5171\n",
      "Step 200, loss = 1.4414\n",
      "Step 300, loss = 0.5110\n",
      "Step 400, loss = 0.0211\n",
      "Step 500, loss = 0.0082\n",
      "Step 600, loss = 0.0065\n",
      "Step 700, loss = 0.0059\n",
      "Step 800, loss = 0.0026\n",
      "Step 900, loss = 0.0018\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = TinyGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    max_seq_len=block_size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):\n",
    "    # 1. get batch\n",
    "    x, y = get_batch(\"train\", dataset, block_size=block_size, device=device)\n",
    "\n",
    "    # 2. forward\n",
    "    logits = model(x)\n",
    "\n",
    "    # 3. compute loss\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "    # 4. backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140fc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    start: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust autoregressive sampling for TinyGPT-like models.\n",
    "\n",
    "    - model: your TinyGPT instance (should have model.pos_emb)\n",
    "    - tokenizer: your CharTokenizer with encode()/decode()\n",
    "    - start: prompt string\n",
    "    - max_new_tokens: how many tokens to generate\n",
    "    - temperature: >0 float (small <1 => deterministic), 0 => greedy\n",
    "    - top_k: keep only top_k logits (int) if not None\n",
    "    - top_p: nucleus probability threshold (0<p<1) if not None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    # encode prompt\n",
    "    input_ids = tokenizer.encode(start)\n",
    "    if len(input_ids) == 0:\n",
    "        raise ValueError(\"Prompt must be non-empty (or handle a default token).\")\n",
    "    # make tensor shape (1, seq)\n",
    "    generated = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # max context length supported by the model's positional embeddings\n",
    "    # we read num_embeddings (safe even if model doesn't store max_seq_len explicitly)\n",
    "    max_context = model.pos_emb.num_embeddings\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # trim to model context window (use only the last max_context tokens)\n",
    "        if generated.size(1) > max_context:\n",
    "            input_ids_tensor = generated[:, -max_context:]\n",
    "        else:\n",
    "            input_ids_tensor = generated\n",
    "\n",
    "        # forward pass to get logits: shape (1, seq, vocab)\n",
    "        logits = model(input_ids_tensor)  \n",
    "        logits = logits[:, -1, :]  # take logits for the last position -> shape (1, vocab)\n",
    "\n",
    "        # temperature handling\n",
    "        if temperature == 0:\n",
    "            # greedy\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)  # shape (1,1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            continue\n",
    "        else:\n",
    "            logits = logits / float(temperature)\n",
    "\n",
    "        # top-k filtering (optional)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))  # don't exceed vocab\n",
    "            # get the kth largest logit value for each batch row and mask below it\n",
    "            values, _ = torch.topk(logits, top_k, dim=-1)\n",
    "            min_values = values[..., -1, None]  # threshold\n",
    "            logits = torch.where(logits < min_values, torch.full_like(logits, float(\"-inf\")), logits)\n",
    "\n",
    "        # top-p (nucleus) filtering (optional)\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "            # mask tokens with cumulative prob > top_p\n",
    "            sorted_mask = cumulative_probs > top_p\n",
    "            # keep at least one token: shift mask right so first token included\n",
    "            sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "            sorted_mask[..., 0] = False\n",
    "\n",
    "            # now set logits of tokens to remove to -inf\n",
    "            indices_to_remove = sorted_mask.scatter(-1, sorted_indices, sorted_mask)\n",
    "            logits = logits.masked_fill(indices_to_remove, float(\"-inf\"))\n",
    "\n",
    "        # convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # (1, vocab)\n",
    "\n",
    "        # sample\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # return the entire sequence as text (prompt + generated)\n",
    "    return tokenizer.decode(generated[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796d41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n"
     ]
    }
   ],
   "source": [
    "text = sample(model, tokenizer=tok, start=\"Hello\", max_new_tokens=50,\n",
    "              temperature=0.8, top_k=20, top_p=0.9, device=\"cpu\")\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
