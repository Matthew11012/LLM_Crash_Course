{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6316e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import TinyGPT\n",
    "from data import CharDataset, CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d16e6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394\n",
      "Sample:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "if not os.path.exists(\"input.txt\"):\n",
    "    with open(\"input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Dataset length:\", len(text))\n",
    "print(\"Sample:\\n\", text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5aab2",
   "metadata": {},
   "source": [
    "### Create BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, normalizers\n",
    "\n",
    "# Create a BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(), # splits accented letters into base + accent mark\n",
    "    normalizers.StripAccents() # removes the accent mark\n",
    "])\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "tokenizer.train(files=[\"input.txt\"], trainer=trainer)\n",
    "\n",
    "# Save and reload\n",
    "tokenizer.save(\"bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8bf2f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠH', 'ello', 'Ġworld', '!']\n",
      "[141, 3709, 696, 2]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "encoded = tokenizer.encode(\"Hello world!\")\n",
    "print(encoded.tokens)   # subword tokens\n",
    "print(encoded.ids)      # corresponding IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa97764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(ids):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "# Encode full dataset\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/val split\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "dataset = {\n",
    "    \"train\": train_data,\n",
    "    \"val\": val_data\n",
    "}\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3618f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example toy dataset\n",
    "# text = \"abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# tok = CharTokenizer(text)\n",
    "# print(\"Vocab size:\", tok.vocab_size)\n",
    "\n",
    "# # Encode the full text into tokens\n",
    "# data = torch.tensor(tok.encode(text), dtype=torch.long)\n",
    "\n",
    "# # 90% train, 10% val split\n",
    "# n = int(0.9 * len(data))\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]\n",
    "\n",
    "# dataset = {\n",
    "#     \"train\": train_data,\n",
    "#     \"val\": val_data\n",
    "# }\n",
    "\n",
    "# block_size = 8\n",
    "# vocab_size = tok.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "439a9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, data, batch_size=4, block_size=8, device=\"cuda\"):\n",
    "    # split: \"train\" or \"val\"\n",
    "    # data: dict with {\"train\": tensor, \"val\": tensor}\n",
    "    \n",
    "    # pick the right dataset\n",
    "    data_split = data[split]\n",
    "\n",
    "    # pick random starting indices\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    \n",
    "    # slice out input (x) and target (y)\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f58ff504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, split, batch_size=32, block_size=8, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(100):  # sample 100 mini-batches for val\n",
    "        x, y = get_batch(split, data, batch_size=batch_size, block_size=block_size, device=device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c1d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Step 0, train loss = 85.9706, val loss = 84.1309, val ppl = 3448178346347383671907880052107247616.00\n",
      "Step 50, train loss = 29.7327, val loss = 28.0439, val ppl = 1511179072050.17\n",
      "Step 100, train loss = 22.0310, val loss = 20.8129, val ppl = 1093793479.46\n",
      "Step 150, train loss = 18.4586, val loss = 17.4859, val ppl = 39268022.10\n",
      "Step 200, train loss = 14.2242, val loss = 15.2872, val ppl = 4356382.78\n",
      "Step 250, train loss = 11.9233, val loss = 13.7279, val ppl = 916148.13\n",
      "Step 300, train loss = 12.8864, val loss = 12.5391, val ppl = 279025.76\n",
      "Step 350, train loss = 10.2429, val loss = 11.6767, val ppl = 117799.57\n",
      "Step 400, train loss = 13.2997, val loss = 11.1662, val ppl = 70702.62\n",
      "Step 450, train loss = 12.7010, val loss = 10.4171, val ppl = 33425.61\n",
      "Step 500, train loss = 9.0996, val loss = 9.9958, val ppl = 21934.90\n",
      "Step 550, train loss = 8.8640, val loss = 9.5351, val ppl = 13836.67\n",
      "Step 600, train loss = 6.8515, val loss = 9.1199, val ppl = 9135.46\n",
      "Step 650, train loss = 7.8816, val loss = 8.6161, val ppl = 5519.79\n",
      "Step 700, train loss = 8.4156, val loss = 8.1753, val ppl = 3552.05\n",
      "Step 750, train loss = 7.7831, val loss = 7.7308, val ppl = 2277.48\n",
      "Step 800, train loss = 8.7567, val loss = 7.3372, val ppl = 1536.38\n",
      "Step 850, train loss = 8.5132, val loss = 7.0254, val ppl = 1124.83\n",
      "Step 900, train loss = 6.5353, val loss = 6.4865, val ppl = 656.23\n",
      "Step 950, train loss = 4.9269, val loss = 6.0597, val ppl = 428.25\n",
      "Step 1000, train loss = 5.7483, val loss = 5.5373, val ppl = 253.98\n",
      "Step 1050, train loss = 6.3720, val loss = 5.0671, val ppl = 158.72\n",
      "Step 1100, train loss = 7.7699, val loss = 4.6391, val ppl = 103.45\n",
      "Step 1150, train loss = 4.2053, val loss = 4.1499, val ppl = 63.43\n",
      "Step 1200, train loss = 3.4574, val loss = 3.7788, val ppl = 43.76\n",
      "Step 1250, train loss = 3.9969, val loss = 3.3687, val ppl = 29.04\n",
      "Step 1300, train loss = 2.4635, val loss = 2.9271, val ppl = 18.67\n",
      "Step 1350, train loss = 4.1476, val loss = 2.5762, val ppl = 13.15\n",
      "Step 1400, train loss = 1.3657, val loss = 2.2363, val ppl = 9.36\n",
      "Step 1450, train loss = 1.7528, val loss = 1.9431, val ppl = 6.98\n",
      "Step 1500, train loss = 1.8719, val loss = 1.7246, val ppl = 5.61\n",
      "Step 1550, train loss = 1.0858, val loss = 1.6108, val ppl = 5.01\n",
      "Step 1600, train loss = 1.2191, val loss = 1.5100, val ppl = 4.53\n",
      "Step 1650, train loss = 1.1183, val loss = 1.4635, val ppl = 4.32\n",
      "Step 1700, train loss = 0.6663, val loss = 1.4123, val ppl = 4.11\n",
      "Step 1750, train loss = 1.3509, val loss = 1.3326, val ppl = 3.79\n",
      "Step 1800, train loss = 0.9469, val loss = 1.2838, val ppl = 3.61\n",
      "Step 1850, train loss = 0.7763, val loss = 1.2339, val ppl = 3.43\n",
      "Step 1900, train loss = 1.3781, val loss = 1.2208, val ppl = 3.39\n",
      "Step 1950, train loss = 0.5423, val loss = 1.1852, val ppl = 3.27\n",
      "Step 2000, train loss = 1.3184, val loss = 1.1582, val ppl = 3.18\n",
      "Step 2050, train loss = 0.9621, val loss = 1.1610, val ppl = 3.19\n",
      "Step 2100, train loss = 1.2969, val loss = 1.1579, val ppl = 3.18\n",
      "Step 2150, train loss = 1.0214, val loss = 1.1325, val ppl = 3.10\n",
      "Step 2200, train loss = 0.9180, val loss = 1.0837, val ppl = 2.96\n",
      "Step 2250, train loss = 0.7332, val loss = 1.1102, val ppl = 3.03\n",
      "Step 2300, train loss = 0.8793, val loss = 1.0838, val ppl = 2.96\n",
      "Step 2350, train loss = 0.8855, val loss = 1.0553, val ppl = 2.87\n",
      "Step 2400, train loss = 0.4703, val loss = 1.0363, val ppl = 2.82\n",
      "Step 2450, train loss = 1.1842, val loss = 1.0473, val ppl = 2.85\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import csv\n",
    "\n",
    "device = \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = TinyGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    max_seq_len=block_size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "with open(\"metrics/metrics.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"step\", \"train_loss\", \"val_loss\", \"val_ppl\"])\n",
    "\n",
    "# Training loop\n",
    "for step in range(2500):\n",
    "    # 1. get batch\n",
    "    x, y = get_batch(\"train\", dataset, block_size=block_size, device=device)\n",
    "\n",
    "    # 2. forward\n",
    "    logits = model(x)\n",
    "\n",
    "    # 3. compute loss\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "    # 4. backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        val_loss = evaluate(model, dataset, \"val\", block_size=block_size, device=device)\n",
    "        train_loss = loss.item()\n",
    "        with open(\"metrics/metrics.csv\", \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([step, train_loss, val_loss, math.exp(val_loss)])\n",
    "        print(f\"Step {step}, train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val ppl = {math.exp(val_loss):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "140fc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    start,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust autoregressive sampling for TinyGPT-like models.\n",
    "\n",
    "    - model: your TinyGPT instance (should have model.pos_emb)\n",
    "    - tokenizer: your CharTokenizer with encode()/decode()\n",
    "    - start: prompt string\n",
    "    - max_new_tokens: how many tokens to generate\n",
    "    - temperature: >0 float (small <1 => deterministic), 0 => greedy\n",
    "    - top_k: keep only top_k logits (int) if not None\n",
    "    - top_p: nucleus probability threshold (0<p<1) if not None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    # encode prompt\n",
    "    if isinstance(start, str):\n",
    "        input_ids = tokenizer.encode(start).ids\n",
    "    elif isinstance(start, list):\n",
    "        input_ids = start\n",
    "    else:\n",
    "        raise ValueError(\"start must be a string or list of token IDs\")\n",
    "    \n",
    "    if len(input_ids) == 0:\n",
    "        raise ValueError(\"Prompt must be non-empty (or handle a default token).\")\n",
    "    # make tensor shape (1, seq)\n",
    "    generated = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # max context length supported by the model's positional embeddings\n",
    "    # we read num_embeddings (safe even if model doesn't store max_seq_len explicitly)\n",
    "    max_context = model.pos_emb.num_embeddings\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # trim to model context window (use only the last max_context tokens)\n",
    "        if generated.size(1) > max_context:\n",
    "            input_ids_tensor = generated[:, -max_context:]\n",
    "        else:\n",
    "            input_ids_tensor = generated\n",
    "\n",
    "        # forward pass to get logits: shape (1, seq, vocab)\n",
    "        logits = model(input_ids_tensor)  \n",
    "        logits = logits[:, -1, :]  # take logits for the last position -> shape (1, vocab)\n",
    "\n",
    "        # temperature handling\n",
    "        if temperature == 0:\n",
    "            # greedy\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)  # shape (1,1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            continue\n",
    "        else:\n",
    "            logits = logits / float(temperature)\n",
    "\n",
    "        # top-k filtering (optional)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))  # don't exceed vocab\n",
    "            # get the kth largest logit value for each batch row and mask below it\n",
    "            values, _ = torch.topk(logits, top_k, dim=-1)\n",
    "            min_values = values[..., -1, None]  # threshold\n",
    "            logits = torch.where(logits < min_values, torch.full_like(logits, float(\"-inf\")), logits)\n",
    "\n",
    "        # top-p (nucleus) filtering (optional)\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "            # mask tokens with cumulative prob > top_p\n",
    "            sorted_mask = cumulative_probs > top_p\n",
    "            # keep at least one token: shift mask right so first token included\n",
    "            sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "            sorted_mask[..., 0] = False\n",
    "\n",
    "            # now set logits of tokens to remove to -inf\n",
    "            indices_to_remove = sorted_mask.scatter(-1, sorted_indices, sorted_mask)\n",
    "            logits = logits.masked_fill(indices_to_remove, float(\"-inf\"))\n",
    "\n",
    "        # convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # (1, vocab)\n",
    "\n",
    "        # sample\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # return the entire sequence as text (prompt + generated)\n",
    "    return tokenizer.decode(generated[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "796d41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are all resolved what what what what what what what what what what what what what what what what what what what what what what what what what what what what what what\n"
     ]
    }
   ],
   "source": [
    "start_ids = tokenizer.encode(\"You are all resolved\").ids\n",
    "\n",
    "text = sample(model, tokenizer=tokenizer, start=start_ids, max_new_tokens=30,\n",
    "              temperature=0.8, top_k=20, top_p=0.9, device=\"cpu\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b983eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠH', 'ello', 'Ġworld', '!']\n",
      " Hello world!\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Hello world!\")\n",
    "print(encoded.tokens)  # ['ĠH', 'ello', 'Ġworld', '!']\n",
    "print(tokenizer.decode(encoded.ids))  # Hello world!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
