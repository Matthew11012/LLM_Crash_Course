{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9a4f1e",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346758a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base model output ===\n",
      "Explain why the sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "prompt = \"Explain why the sky is blue\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate without LoRA\n",
    "with torch.no_grad():\n",
    "    base_out = base_model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(\"=== Base model output ===\")\n",
    "print(tokenizer.decode(base_out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be91989",
   "metadata": {},
   "source": [
    "### LoRA Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844edb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model with LoRA output ===\n",
      "Explain why the sky is blue and why it's blue.\n",
      "\n",
      "The sky is blue because it is blue. It is blue\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model again\n",
    "model_with_lora = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Attach LoRA adapter\n",
    "model_with_lora = PeftModel.from_pretrained(model_with_lora, \"outputs/lora_adapter\")\n",
    "model_with_lora.to(device)\n",
    "\n",
    "# Generate with LoRA\n",
    "with torch.no_grad():\n",
    "    lora_out = model_with_lora.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(\"\\n=== Model with LoRA output ===\")\n",
    "print(tokenizer.decode(lora_out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044143b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain why the sky is blue\n",
      "\n",
      "=== Base model output ===\n",
      "Explain why the sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "=== Model with LoRA output ===\n",
      "Explain why the sky is blue and why it's blue.\n",
      "\n",
      "The sky is blue because it is blue. It is blue because it is blue because it is blue because it is blue because it is blue because it is blue because it is blue because it is blue because it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: List the steps for making a peanut butter and jelly sandwich\n",
      "\n",
      "=== Base model output ===\n",
      "List the steps for making a peanut butter and jelly sandwich.\n",
      "\n",
      "Step 1:\n",
      "\n",
      "1. In a large bowl, combine the butter, sugar, and salt.\n",
      "\n",
      "2. Add the eggs, milk, and vanilla.\n",
      "\n",
      "3. Add the flour and mix well.\n",
      "\n",
      "\n",
      "\n",
      "=== Model with LoRA output ===\n",
      "List the steps for making a peanut butter and jelly sandwich.\n",
      "\n",
      "Step 1: Preheat oven to 350 degrees F.\n",
      "\n",
      "Step 2: In a large bowl, whisk together the butter, sugar, and vanilla.\n",
      "\n",
      "Step 3: Add the eggs, vanilla, and salt.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a short story about a robot who learns to paint.\n",
      "\n",
      "=== Base model output ===\n",
      "Write a short story about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to\n",
      "\n",
      "=== Model with LoRA output ===\n",
      "Write a short story about a robot who learns to paint.\n",
      "\n",
      "\"I'm a robot,\" says the robot, \"and I'm learning to paint.\"\n",
      "\n",
      "\"I'm learning to paint,\" says the robot, \"and I'm learning to paint.\"\n",
      "\n",
      "\"I'm learning to paint,\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Compare cats and dogs as pets in a few sentences.\n",
      "\n",
      "=== Base model output ===\n",
      "Compare cats and dogs as pets in a few sentences.\n",
      "\n",
      "\"I'm not going to say that I'm a cat lover, but I'm not going to say that I'm a dog lover either,\" he said. \"I'm not going to say that I'm a dog lover either.\"\n",
      "\n",
      "\n",
      "=== Model with LoRA output ===\n",
      "Compare cats and dogs as pets in a few sentences.\n",
      "\n",
      "\"I'm not sure if it's a good idea to have cats or dogs in your home, but I think it's a good idea to have them in your home,\" she said.\n",
      "\n",
      "\"I think it's important to have\n"
     ]
    }
   ],
   "source": [
    "prompts=[\n",
    "    \"Explain why the sky is blue\",\n",
    "    \"List the steps for making a peanut butter and jelly sandwich\",\n",
    "    \"Write a short story about a robot who learns to paint.\",\n",
    "    \"Compare cats and dogs as pets in a few sentences.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate without LoRA\n",
    "    with torch.no_grad():\n",
    "        base_out = base_model.generate(**inputs, max_new_tokens=50)\n",
    "        lora_out = model_with_lora.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=== Base model output ===\")\n",
    "    print(tokenizer.decode(base_out[0], skip_special_tokens=True))\n",
    "    print(\"\\n=== Model with LoRA output ===\")\n",
    "    print(tokenizer.decode(lora_out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
