{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9a4f1e",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346758a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\Coding\\LLM\\LLM_Crash_Course\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0904 16:04:16.768000 20208 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base model output ===\n",
      "Explain why the sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "prompt = \"Explain why the sky is blue\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate without LoRA\n",
    "with torch.no_grad():\n",
    "    base_out = base_model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(\"=== Base model output ===\")\n",
    "print(tokenizer.decode(base_out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be91989",
   "metadata": {},
   "source": [
    "### LoRA Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844edb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model with LoRA output ===\n",
      "Explain why the sky is blue and why it's blue.\n",
      "\n",
      "The sky is blue because it is a complex system of colors\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model again\n",
    "model_with_lora = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Attach LoRA adapter\n",
    "model_with_lora = PeftModel.from_pretrained(model_with_lora, \"outputs/lora_adapter\")\n",
    "model_with_lora.to(device)\n",
    "\n",
    "# Generate with LoRA\n",
    "with torch.no_grad():\n",
    "    lora_out = model_with_lora.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(\"\\n=== Model with LoRA output ===\")\n",
    "print(tokenizer.decode(lora_out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044143b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Explain why the sky is blue\n",
      "KL divergence: 0.228882\n",
      "L2 distance:   3722.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base model output ===\n",
      "Explain why the sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "The sky is blue.\n",
      "\n",
      "=== LoRA model output ===\n",
      "Explain why the sky is blue and why it's blue.\n",
      "\n",
      "The sky is blue because it is a complex system of colors, which means that it is composed of many different shades of blue, including red, green, and blue. The colors of the sky are also complex\n",
      "\n",
      "Prompt: List the steps for making a peanut butter and jelly sandwich\n",
      "KL divergence: 0.120178\n",
      "L2 distance:   5072.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base model output ===\n",
      "List the steps for making a peanut butter and jelly sandwich.\n",
      "\n",
      "Step 1:\n",
      "\n",
      "1. In a large bowl, combine the butter, sugar, and salt.\n",
      "\n",
      "2. Add the eggs, milk, and vanilla.\n",
      "\n",
      "3. Add the flour and mix well.\n",
      "\n",
      "\n",
      "\n",
      "=== LoRA model output ===\n",
      "List the steps for making a peanut butter and jelly sandwich.\n",
      "\n",
      "1. Preheat oven to 350 degrees F.\n",
      "\n",
      "2. In a large bowl, whisk together the butter, sugar, and eggs.\n",
      "\n",
      "3. Pour the mixture into a large bowl and whisk until smooth.\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Write a short story about a robot who learns to paint.\n",
      "KL divergence: 0.407227\n",
      "L2 distance:   8680.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base model output ===\n",
      "Write a short story about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to paint.\n",
      "\n",
      "The story is about a robot who learns to\n",
      "\n",
      "=== LoRA model output ===\n",
      "Write a short story about a robot who learns to paint.\n",
      "\n",
      "\"I'm a robot, and I'm not a painter,\" says the robot, who is wearing a white suit and a blue tie. \"I'm a robot, and I'm not a painter.\"\n",
      "\n",
      "The robot is a robot\n",
      "\n",
      "Prompt: Compare cats and dogs as pets in a few sentences.\n",
      "KL divergence: 0.207520\n",
      "L2 distance:   8080.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base model output ===\n",
      "Compare cats and dogs as pets in a few sentences.\n",
      "\n",
      "\"I'm not going to say that I'm a cat lover, but I'm not going to say that I'm a dog lover either,\" he said. \"I'm not going to say that I'm a dog lover either.\"\n",
      "\n",
      "\n",
      "=== LoRA model output ===\n",
      "Compare cats and dogs as pets in a few sentences.\n",
      "\n",
      "\"I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm\n",
      "\n",
      "Prompt: Create a story about a robot that falls in love with a human.\n",
      "KL divergence: 0.347168\n",
      "L2 distance:   11000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base model output ===\n",
      "Create a story about a robot that falls in love with a human.\n",
      "\n",
      "The story is about a robot that falls in love with a human.\n",
      "\n",
      "The story is about a robot that falls in love with a human.\n",
      "\n",
      "The story is about a robot that falls in love with a human.\n",
      "\n",
      "\n",
      "\n",
      "=== LoRA model output ===\n",
      "Create a story about a robot that falls in love with a human.\n",
      "\n",
      "The robot, named \"Bumblebee,\" is a robotic robot that has been programmed to perform tasks such as picking up objects and moving them around. It's also capable of making complex calculations and making complex calculations on its own.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "prompts = [\n",
    "    \"Explain why the sky is blue\",\n",
    "    \"List the steps for making a peanut butter and jelly sandwich\",\n",
    "    \"Write a short story about a robot who learns to paint.\",\n",
    "    \"Compare cats and dogs as pets in a few sentences.\",\n",
    "    \"Create a story about a robot that falls in love with a human.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get raw logits for base and LoRA models\n",
    "        base_logits = base_model(**inputs).logits[:, -1, :]   # [batch, vocab]\n",
    "        lora_logits = model_with_lora(**inputs).logits[:, -1, :]\n",
    "\n",
    "    # Normalize to probability distributions\n",
    "    base_probs = F.softmax(base_logits, dim=-1)\n",
    "    lora_probs = F.softmax(lora_logits, dim=-1)\n",
    "\n",
    "    lora_log_probs = F.log_softmax(lora_logits, dim=-1)\n",
    "\n",
    "    # Compute metrics\n",
    "    kl_div = F.kl_div(\n",
    "        lora_log_probs,  # log(P)\n",
    "        base_probs,        # Q\n",
    "        reduction=\"batchmean\"\n",
    "    ).item()\n",
    "\n",
    "    l2_dist = torch.norm(base_logits - lora_logits, p=2).item()\n",
    "\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"KL divergence: {kl_div:.6f}\")\n",
    "    print(f\"L2 distance:   {l2_dist:.6f}\")\n",
    "\n",
    "    # (Optional) also print text generations like before\n",
    "    base_out = base_model.generate(**inputs, max_new_tokens=50)\n",
    "    lora_out = model_with_lora.generate(**inputs, max_new_tokens=50)\n",
    "    print(\"\\n=== Base model output ===\")\n",
    "    print(tokenizer.decode(base_out[0], skip_special_tokens=True))\n",
    "    print(\"\\n=== LoRA model output ===\")\n",
    "    print(tokenizer.decode(lora_out[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
